{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x000002073B556648>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/selenium/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x000002073B556FC8>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/selenium/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x000002073B556E48>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/selenium/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x000002073B564A08>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/selenium/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x000002073B564C88>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/selenium/\n",
      "ERROR: Could not find a version that satisfies the requirement selenium (from versions: none)\n",
      "ERROR: No matching distribution found for selenium\n"
     ]
    }
   ],
   "source": [
    "# https://pypi.org/project/selenium/\n",
    "# https://selenium-python.readthedocs.io/api.html\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython in c:\\users\\debba\\anaconda3\\lib\\site-packages (7.12.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\debba\\anaconda3\\lib\\site-packages (from ipython) (0.4.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\debba\\anaconda3\\lib\\site-packages (from ipython) (45.2.0.post20200210)\n",
      "Requirement already satisfied: pygments in c:\\users\\debba\\anaconda3\\lib\\site-packages (from ipython) (2.5.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\debba\\anaconda3\\lib\\site-packages (from ipython) (4.4.1)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\debba\\anaconda3\\lib\\site-packages (from ipython) (0.16.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\debba\\anaconda3\\lib\\site-packages (from ipython) (4.3.3)\n",
      "Requirement already satisfied: backcall in c:\\users\\debba\\anaconda3\\lib\\site-packages (from ipython) (0.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\debba\\anaconda3\\lib\\site-packages (from ipython) (3.0.3)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\debba\\anaconda3\\lib\\site-packages (from ipython) (0.7.5)\n",
      "Requirement already satisfied: parso>=0.5.2 in c:\\users\\debba\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython) (0.6.1)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\debba\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython) (0.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\debba\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython) (1.14.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\debba\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.1.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\debba\\anaconda3\\lib\\site-packages (4.8.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\debba\\anaconda3\\lib\\site-packages (from beautifulsoup4) (1.9.5)\n"
     ]
    }
   ],
   "source": [
    "# https://pypi.org/project/beautifulsoup4/\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install csv\n",
    "pip install time \n",
    "pip install parsel\n",
    "#pip install lxml\n",
    "pip install bs4\n",
    "pip install html5lib\n",
    "pip install et_xmlfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lxml\n",
    "import requests, time, random\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome('C:/driver/chromedriver_win32/chromedriver.exe')\n",
    "browser.get('https://www.linkedin.com/uas/login')\n",
    "file = open('C:/driver/config.txt')\n",
    "lines = file.readlines()\n",
    "username = lines[0]\n",
    "password = lines[1]\n",
    "\n",
    "\n",
    "elementID = browser.find_element_by_id('username')\n",
    "elementID.send_keys(username)\n",
    "\n",
    "elementID = browser.find_element_by_id('password')\n",
    "elementID.send_keys(password)\n",
    "\n",
    "#elementID.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing fonction Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HTML_Sraping(link):\n",
    "   \n",
    "    browser.get(link)\n",
    "    res=requests.get(link)\n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "# Get scroll height\n",
    "    y = 500\n",
    "    y1 = 500\n",
    "    # Get scroll height\n",
    "    z = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        #pass\n",
    "    for i in range(0,30):\n",
    "        # Scroll down to bottom\n",
    "        browser.execute_script(\"window.scrollTo(0, \"+str(y)+\")\")\n",
    "        y += random.randint(300, 600) \n",
    "        time.sleep(0.1)\n",
    "        # Wait to load page\n",
    "        #time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height <= y:\n",
    "            break\n",
    "    for timer in range(0,30):\n",
    "        browser.execute_script(\"window.scrollTo(0, \"+str(z-y1)+\")\")\n",
    "        y1 += random.randint(500, 600) \n",
    "        time.sleep(0.1)\n",
    "        if z <= y1/2:\n",
    "            break         \n",
    "    try:\n",
    "        browser.find_element_by_css_selector(\"button[data-control-name='skill_details']\").click() \n",
    "        #print(\"clicked\")\n",
    "    except:\n",
    "        pass\n",
    "    src = browser.page_source\n",
    "    #soup = BeautifulSoup(src, 'lxml')\n",
    "    soup = BeautifulSoup(src,'lxml')\n",
    "    #soup = src\n",
    "    return soup    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Json_Scraping_2(link):\n",
    "    soup=HTML_Sraping(link)\n",
    "    final_liste=[]\n",
    "    info=dict()\n",
    "    info[\"link\"]=[link]\n",
    "     ###########Personal info_name\n",
    "    try:\n",
    "        \n",
    "        #info ={'link':'Vide','fullName':'Vide','profile_title':'Vide','location':'Vide','connectionsCount':'Vide','job_title':'Vide','company_name':'Vide','joining_date':'Vide','exp_duree':'Vide','college_name':'Vide','degree_name':'Vide','stream':'Vide','degree_year':'Vide','Language':'Vide'}\n",
    "        name_div = soup.find('div', {'class': 'flex-1 mr5'})\n",
    "        name_loc = name_div.find_all('ul')\n",
    "        fullName = name_loc[0].find('li').get_text().strip()    \n",
    "        \n",
    "        info[\"fullName\"]=[fullName]\n",
    "    except:\n",
    "        #print('no data')\n",
    "\n",
    "        pass         \n",
    "         ###########Personal info_Location   \n",
    "    try:    \n",
    "        name_div = soup.find('div', {'class': 'flex-1 mr5'})\n",
    "        name_loc = name_div.find_all('ul')\n",
    "        location = name_loc[1].find('li').get_text().strip()\n",
    "        info[\"location\"]=[location]\n",
    "    except:\n",
    "        #print('no data')\n",
    "\n",
    "        pass \n",
    "         ###########Personal info_Profile_title\n",
    "    try:\n",
    "        name_div = soup.find('div', {'class': 'flex-1 mr5'})\n",
    "        profile_title = name_div.find('h2').get_text().strip()\n",
    "        info[\"profile_title\"]=[profile_title]\n",
    "    except:\n",
    "        #print('no data')\n",
    "\n",
    "        pass     \n",
    "\n",
    "          ###########Personal info_ConnectionsCount\n",
    "    try:\n",
    "        name_div = soup.find('div', {'class': 'flex-1 mr5'})\n",
    "        name_loc = name_div.find_all('ul')\n",
    "        name_loc = name_div.find_all('ul')\n",
    "        connection = name_loc[1].find_all('li')\n",
    "        connectionsCount = connection[1].get_text().strip()\n",
    "\n",
    "        info[\"connectionsCount\"]=[connectionsCount]\n",
    "    except:\n",
    "        #print('no data')\n",
    "\n",
    "        pass     \n",
    "\n",
    "    ############## Experience\n",
    "    try:\n",
    "        exp_section = soup.find('section', {'id': 'experience-section'}).find('ul')\n",
    "        job_title=[]\n",
    "        jo=[]\n",
    "        company_name=[]\n",
    "        joining_date=[]\n",
    "        exp_duree=[]\n",
    "        for i in range(len(exp_section.find_all('div', {'class': \"display-flex flex-column full-width\"}))):\n",
    "            a_tags=exp_section.find_all('div', {'class': \"display-flex flex-column full-width\"})[i].find('a')\n",
    "            jo.append(a_tags.find('h3').get_text().strip())\n",
    "            company_name .append( a_tags.find_all('p')[1].get_text().strip())\n",
    "            joining_date .append(a_tags.find_all('h4')[0].find_all('span')[1].get_text().strip())\n",
    "            exp_duree .append( a_tags.find_all('h4')[1].find_all('span')[1].get_text().strip())\n",
    "            info[\"job_title\"]=[jo]\n",
    "            info[\"company_name\"]=[company_name] \n",
    "            info[\"joining_date\"]=[joining_date]\n",
    "            info[\"exp_duree\"]=[exp_duree]\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass     \n",
    "\n",
    "    #############Education( college_name)\n",
    "    try:\n",
    "        edu_section = soup.find('section', {'id': 'education-section'}).find('ul')\n",
    "        college_name=[]\n",
    "\n",
    "        for i in range(len(edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"}))):\n",
    "            edu=edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"})[i]\n",
    "            college_name.append(edu.find('h3').get_text().strip())\n",
    "            info[\"college_name\"]=[college_name]\n",
    "\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass        \n",
    "    #############Education( degree_name)\n",
    "    try:\n",
    "        edu_section = soup.find('section', {'id': 'education-section'}).find('ul')\n",
    "    \n",
    "        degree_name=[]\n",
    "   \n",
    "        for i in range(len(edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"}))):\n",
    "            edu=edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"})[i]\n",
    "            \n",
    "            degree_name.append(edu.find('p', {'class': 'pv-entity__secondary-title pv-entity__degree-name t-14 t-black t-normal'}).find_all('span')[1].get_text().strip())\n",
    "           \n",
    "            info[\"degree_name\"]=[degree_name]\n",
    "            \n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass\n",
    "    #############Education(stream)\n",
    "    try:\n",
    "        edu_section = soup.find('section', {'id': 'education-section'}).find('ul')\n",
    "        \n",
    "        stream =[]\n",
    "        degree_year=[]\n",
    "        for i in range(len(edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"}))):\n",
    "            edu=edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"})[i]\n",
    "            \n",
    "            stream.append(edu.find('p', {'class': 'pv-entity__secondary-title pv-entity__fos t-14 t-black t-normal'}).find_all('span')[1].get_text().strip()) \n",
    "            info[\"stream\"]=[stream]\n",
    "            \n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass\n",
    "    #############Education(degree_year)\n",
    "    try:\n",
    "        edu_section = soup.find('section', {'id': 'education-section'}).find('ul')\n",
    "        degree_year=[]\n",
    "        for i in range(len(edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"}))):\n",
    "            edu=edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"})[i]\n",
    "            degree_year.append( edu.find('p', {'class': 'pv-entity__dates t-14 t-black--light t-normal'}).find_all('span')[1].get_text().strip())\n",
    "            info[\"degree_year\"]=[degree_year]\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass     \n",
    "    #######Endorsements\n",
    "    try:\n",
    "        en=[]\n",
    "        for i in range(len(soup.find_all('span', {'class':'custom-highlight t-bold'}))):\n",
    "            en.append(soup.find_all('span', {'class':'custom-highlight t-bold'})[i].get_text().strip())\n",
    "            info[\"endorsements\"]=[en]  \n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass         \n",
    "\n",
    "    ######Skills\n",
    "    try:\n",
    "        ss=[]\n",
    "        for i in range(len(soup.find_all('div', {'class':'pv-skill-category-entity__skill-wrapper'}))):\n",
    "            ss.append(soup.find_all('div', {'class':'pv-skill-category-entity__skill-wrapper'})[i].find('p',{'class':'pv-skill-category-entity__name tooltip-container'}).get_text().strip())\n",
    "            info[\"skills\"]=[ss]\n",
    "    except:\n",
    "        print('no data')\n",
    "\n",
    "        pass\n",
    "    ######Skills(Outils et technologie)\n",
    "    try:\n",
    "        ss=[]\n",
    "        for i in range(len(soup.find_all('div', {'class':'pv-skill-category-entity__skill-wrapper'}))):\n",
    "            ss.append(soup.find_all('div', {'class':'pv-skill-category-entity__skill-wrapper'})[i].find('p',{'class':'pv-skill-category-entity__name tooltip-container'}).get_text().strip())\n",
    "            info[\"skills\"]=[ss]\n",
    "    except:\n",
    "        #print('no data')\n",
    "\n",
    "        pass     \n",
    " \n",
    "    #####Accomplishments\n",
    "    ####Language\n",
    "    try:\n",
    "        Accomp_section = soup.find('section', {'class': 'accordion-panel pv-profile-section pv-accomplishments-block languages ember-view'})        \n",
    "\n",
    "        ll=[]\n",
    "        for i in range(len(Accomp_section.find('ul').find_all('li'))):\n",
    "            ll.append((Accomp_section.find('ul').find_all('li'))[i].get_text().strip())\n",
    "            info[\"Language\"]=[ll]\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass \n",
    "    ######Projects\n",
    "    try:\n",
    "        pp=[]\n",
    "        projets_section=soup.find('section', {'class': 'accordion-panel pv-profile-section pv-accomplishments-block projects ember-view'})        \n",
    "\n",
    "        projets_section.find_all('li', {'class':\"pv-accomplishments-block__summary-list-item\"})\n",
    "        for i in range(len(projets_section.find_all('li', {'class':\"pv-accomplishments-block__summary-list-item\"}))):\n",
    "            pp.append(projets_section.find_all('li', {'class':\"pv-accomplishments-block__summary-list-item\"})[i].get_text().strip())\n",
    "            info[\"project_num\"]=[pp]\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass     \n",
    "    ######number of Projects\n",
    "    try:\n",
    "        projets_section=soup.find('section', {'class': 'accordion-panel pv-profile-section pv-accomplishments-block projects ember-view'})        \n",
    "\n",
    "        for i in projets_section.find(\"h3\").find('span', {'class':\"visually-hidden\"}).get_text().strip():        \n",
    "            try:\n",
    "                (int(i))\n",
    "                info[\"nobre de projets\"]=[int(i)]\n",
    "                #print(int(i))\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass     \n",
    " \n",
    "    ######Publications and numbers\n",
    "    try:\n",
    "        publication_section = soup.find('div', {'id':'publications-expandable-content'})\n",
    "        #publication=dict()\n",
    "        p=[]\n",
    "        #nombre=dict()\n",
    "        nb=0\n",
    "        \n",
    "        for i in range(len(soup.find_all('div', {'id':'publications-expandable-content'}))):\n",
    "            p.append(soup.find_all('div', {'id':'publications-expandable-content'})[i].find('li').get_text().strip())\n",
    "            info[\"Publication\"]=[p]\n",
    "            nb=nb+1\n",
    "            info[\"Pub_numbers\"]=[nb]\n",
    "\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass \n",
    "    ######Interests\n",
    "    try:\n",
    "        \n",
    "        interests_section = soup.find('section', {'class':\"pv-profile-section pv-interests-section artdeco-container-card ember-view\"})\n",
    "        interests_name=[]\n",
    "        for i in range(len(interests_section.find_all('div', {'class':\"pv-entity__summary-info ember-view\"}))):\n",
    "            inter=interests_section.find_all('div', {'class':\"pv-entity__summary-info ember-view\"})[i]\n",
    "            interests_name.append(inter.find('span').get_text().strip())\n",
    "            info[\"interests\"]=[interests_name]\n",
    "    except:\n",
    "        print('no data')\n",
    "        pass\n",
    "    ######Certifications\n",
    "    try:\n",
    "        \n",
    "        certif=[]\n",
    "        for i in range(len(soup.find_all('h3', {'class':\"t-16 t-bold\"}))):\n",
    "            certif.append(soup.find_all('h3', {'class':\"t-16 t-bold\"})[i].get_text().strip())\n",
    "            info[\"certifications\"]=[certif]\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass       \n",
    "    time.sleep(1)\n",
    "    final_liste.append(info)\n",
    "   \n",
    "    return(final_liste)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=pd.read_csv(\"C:/Users/debba/Desktop/3/url_scrap.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liste_final_scrapped(urls):\n",
    "    liste=[]\n",
    "    #for i in range(len(urls['url'])):\n",
    "    for i in range(0,100):\n",
    "        liste.append(Json_Scraping_2(urls['url'][i]+\"/\"))\n",
    "\n",
    "    return liste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Frame_final_scrapped(urls):    \n",
    "    liste=liste_final_scrapped(urls)\n",
    "    data_liste=[]\n",
    "    for j in range(len(liste)):\n",
    "        df=pd.DataFrame(liste[j])\n",
    "        data_liste.append(df)\n",
    "    return data_liste    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_final_scrapped(urls):\n",
    "    data_liste=Frame_final_scrapped(urls)\n",
    "    data_final=pd.concat(data_liste, axis=0, ignore_index=True)\n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicked\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "clicked\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "clicked\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "clicked\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n",
      "no data\n"
     ]
    }
   ],
   "source": [
    "data=data_final_scrapped(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
